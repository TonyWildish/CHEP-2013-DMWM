
The Data Aggregation Service (DAS) \cite{DAS} was designed to provide a uniform
access to distributed CMS data-services regardless of their security policies,
implementation details and data storage solution. DAS provides users with the ability to
query underlying data-services via the DAS Query Language (QL). It also aggregates
metadata information from various data-services into common records
suitable for end users. Here we discuss the details of DAS architecture and the
current status of the system.

DAS was built on top of a NoSQL document-oriented database (MongoDB
\cite{MongoDB}). It provides several benefits for the DAS use case: schema-less
storage, embedded query language and very fast read/write
operations.\footnote{Our benchmark showed that MongoDB can sustain 20k doc/s
for reading and 7k doc/s for writing operations, respectively.}
DAS has a modular design based on the MVC architecture \cite{MVC} and performs the
following workflow upon user query:

\begin{itemize}
\item fetch data from underlying data-services via existing APIs
\item store unprocessed data into the cache database
\item process data and perform its transformation to a common data representation
(we unify differences in naming conventions, units and data-formats, etc.)
\item aggregate data on a requested key, e.g. dataset, block, etc.
\item store results into the merge database
\item present results to end-users and provide a set of filters as well as
aggregation functions to perform basic operations for slicing and representing
the data in a form suitable for user tasks
\end{itemize}

This design allowed to develop DAS in a data-agnostic manner. For instance,
data can be fetched from underlying data-services regardless of their data format, e.g.
JSON, XML, CSV, etc. The data transformation was done via an external set of
mappings which was maintaned separately from DAS code development. It also gave
us a few benefits which were not foreseen from the design cycle. For example,
data aggregation on a common entity, e.g. data name, may show any existing discrepancy in
underlying services.

DAS uses an in-house Query Language which was based on entity relationships used by
physicists, see \cite{DAS}. It consists of the following structure:

\begin{verbatim}
<selection keys> <set of conditions> | <filters> | <aggregators>
\end{verbatim}

The selection keys were based on well known entities such as dataset, block, file,
run. The conditions were formed via key=value pairs. DAS-QL
provides a limited set of filters such as grep, unique,
as well as set of common aggregation functions such as
sum, min, max. The former support conditional operators and grouping, while
the latter can be extended via custom map-reduce functions to support more complex
use cases. Therefore to express a question {\it Find me all datasets at a
given site and show only those which have size greater than 50} someone will need to
write a DAS query in the following way:

\begin{verbatim}
dataset site=XYZ | grep dataset.name, dataset.size>50
\end{verbatim}

It turns out that such flexibility was not always clear to some users,
mostly those who were unfamiliar with the DAS-QL syntax. Therefore a further
attempt was made to build native support for keyword queries on top of DAS-QL,
see \cite{DASKWS}.

DAS operates as an intelligent cache in front of CMS data-services. It stores
results into two caches upon a provided query. The raw-cache is used to store results from
data-services {\it as is}, while the merge-cache stores aggregated
records. The lifetime of the records is based on information provided by
data providers via HTTP headers. The record maintenance is done in a lazy
fashion, i.e. upon a new user query expired records are wiped out from the cache,
while new ones come in. DAS server performs many operations in parallel, e.g.
it sends concurrent HTTP requests to underlying data-services, processes and
stores
data using multiple threads, and runs multiple monitoring and pre-fetching
daemons. The server runs on a single 8-core node with 24 GB of memory
required for efficient MongoDB operations\footnote{MongoDB relies on indexes
fitted in RAM to provide its superior speed}.

The discussed modular design, flexible QL and NoSQL storage allows DAS to
aggregate information from distributed data-services without imposing any
requirements on them. DAS is able to deal with different security models,
various APIs, data-formats and naming conventions. Right now it uses dozens of CMS
data-services. Data are aggregated into JSON records based on common entity
keys so that it is possible to see information from multiple
data-services in a single record, e.g. run information comes from DBS, Condition DB and
RunRegistry and is represented as a single JSON document listing information from
three data-services. Daily load on the DAS server is constanly growing and has about 10k
queries/day with $\sim O(10M)$ records going in and out of the DAS cache.

